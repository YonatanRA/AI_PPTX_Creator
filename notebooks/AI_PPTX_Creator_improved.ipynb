{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI PPTX Creator - Improved Version\n",
        "\n",
        "This improved version includes:\n",
        "- Better error handling and validation\n",
        "- Configuration management\n",
        "- Code organization with functions\n",
        "- Security improvements\n",
        "- Better documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Configuration and Setup](#1.-Configuration-and-Setup)\n",
        "2. [Testing GPT4 Connection](#2.-Testing-GPT4-Connection)\n",
        "3. [Loading PDF Files](#3.-Loading-PDF-Files)\n",
        "4. [Embedding Model and Vector Database](#4.-Embedding-Model-and-Vector-Database)\n",
        "5. [Creating RAG Chain](#5.-Creating-RAG-Chain)\n",
        "6. [Generating PPTX Code](#6.-Generating-PPTX-Code)\n",
        "7. [Creating the Presentation](#7.-Creating-the-Presentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration and Setup\n",
        "\n",
        "Load environment variables and configure constants. Using a configuration class makes the code more maintainable and testable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Third-party imports\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration class for AI PPTX Creator.\"\"\"\n",
        "    \n",
        "    # API Configuration\n",
        "    OPENAI_API_KEY: str = None\n",
        "    \n",
        "    # Model Configuration\n",
        "    CHAT_MODEL: str = \"gpt-4-turbo\"\n",
        "    CODE_GEN_MODEL: str = \"gpt-3.5-turbo-instruct\"  # More cost-effective for code generation\n",
        "    CODE_GEN_TEMPERATURE: float = 0.0  # Deterministic output for code\n",
        "    CODE_GEN_MAX_TOKENS: int = 2048  # Increased for longer code\n",
        "    \n",
        "    # Retriever Configuration\n",
        "    RETRIEVER_K: int = 2  # Number of documents to retrieve\n",
        "    RETRIEVER_LAMBDA_MULT: float = 0.25  # MMR diversity parameter\n",
        "    \n",
        "    # Directory Configuration\n",
        "    BASE_DIR: Path = Path(\"..\")\n",
        "    PDF_DIR: Path = BASE_DIR / \"pdfs\"\n",
        "    PPTX_DIR: Path = BASE_DIR / \"pptx\"\n",
        "    CHROMA_DB_DIR: Path = BASE_DIR / \"chroma_db\"\n",
        "    \n",
        "    @classmethod\n",
        "    def load_env(cls) -> None:\n",
        "        \"\"\"Load environment variables and validate configuration.\"\"\"\n",
        "        load_dotenv()\n",
        "        cls.OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "        \n",
        "        if not cls.OPENAI_API_KEY:\n",
        "            raise ValueError(\n",
        "                \"OPENAI_API_KEY not found in environment variables. \"\n",
        "                \"Please check your .env file.\"\n",
        "            )\n",
        "        \n",
        "        # Ensure directories exist\n",
        "        cls.PPTX_DIR.mkdir(exist_ok=True)\n",
        "        \n",
        "        print(\"âœ“ Configuration loaded successfully\")\n",
        "        print(f\"âœ“ PDF directory: {cls.PDF_DIR}\")\n",
        "        print(f\"âœ“ Output directory: {cls.PPTX_DIR}\")\n",
        "\n",
        "# Load configuration\n",
        "Config.load_env()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Testing GPT4 Connection\n",
        "\n",
        "Verify the API connection with error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_llm_connection(test_query: str = \"What is the Suez Canal?\") -> str:\n",
        "    \"\"\"\n",
        "    Test the LLM connection with a simple query.\n",
        "    \n",
        "    Args:\n",
        "        test_query: The test question to ask\n",
        "        \n",
        "    Returns:\n",
        "        The model's response\n",
        "        \n",
        "    Raises:\n",
        "        Exception: If connection fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = ChatOpenAI(model=Config.CHAT_MODEL)\n",
        "        response = model.invoke(test_query)\n",
        "        print(\"âœ“ LLM connection successful\")\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— LLM connection failed: {e}\")\n",
        "        raise\n",
        "\n",
        "# Test the connection\n",
        "response = test_llm_connection()\n",
        "print(f\"\\nResponse preview: {response[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Loading PDF Files\n",
        "\n",
        "Load PDF documents with validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pdf_documents(pdf_dir: Path = Config.PDF_DIR) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load PDF documents from the specified directory.\n",
        "    \n",
        "    Args:\n",
        "        pdf_dir: Path to the directory containing PDF files\n",
        "        \n",
        "    Returns:\n",
        "        List of loaded documents\n",
        "        \n",
        "    Raises:\n",
        "        FileNotFoundError: If PDF directory doesn't exist\n",
        "        ValueError: If no PDF files are found\n",
        "    \"\"\"\n",
        "    if not pdf_dir.exists():\n",
        "        raise FileNotFoundError(f\"PDF directory not found: {pdf_dir}\")\n",
        "    \n",
        "    # Check for PDF files\n",
        "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
        "    if not pdf_files:\n",
        "        raise ValueError(f\"No PDF files found in {pdf_dir}\")\n",
        "    \n",
        "    print(f\"Found {len(pdf_files)} PDF file(s): {[f.name for f in pdf_files]}\")\n",
        "    \n",
        "    # Load documents\n",
        "    loader = PyPDFDirectoryLoader(str(pdf_dir))\n",
        "    pages = loader.load()\n",
        "    \n",
        "    print(f\"âœ“ Loaded {len(pages)} pages from PDF documents\")\n",
        "    return pages\n",
        "\n",
        "# Load PDF documents\n",
        "pages = load_pdf_documents()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Embedding Model and Vector Database\n",
        "\n",
        "Create embeddings and store in ChromaDB for efficient retrieval.\n",
        "\n",
        "**Embeddings** transform text into numerical vectors that capture semantic meaning, enabling similarity search.\n",
        "\n",
        "**ChromaDB** is a vector database optimized for storing and retrieving embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_vector_store(documents: List[Document]) -> Chroma:\n",
        "    \"\"\"\n",
        "    Create or load a ChromaDB vector store from documents.\n",
        "    \n",
        "    Args:\n",
        "        documents: List of documents to embed\n",
        "        \n",
        "    Returns:\n",
        "        Chroma vector store instance\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize embedding model\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        \n",
        "        # Create vector store\n",
        "        vector_store = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=embeddings,\n",
        "            persist_directory=str(Config.CHROMA_DB_DIR)\n",
        "        )\n",
        "        \n",
        "        print(f\"âœ“ Vector store created with {len(documents)} documents\")\n",
        "        return vector_store\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error creating vector store: {e}\")\n",
        "        raise\n",
        "\n",
        "def create_retriever(vector_store: Chroma):\n",
        "    \"\"\"\n",
        "    Create a retriever with Maximal Marginal Relevance (MMR).\n",
        "    \n",
        "    MMR balances relevance and diversity in retrieved documents.\n",
        "    \n",
        "    Args:\n",
        "        vector_store: The vector store to create retriever from\n",
        "        \n",
        "    Returns:\n",
        "        Configured retriever\n",
        "    \"\"\"\n",
        "    retriever = vector_store.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\n",
        "            \"k\": Config.RETRIEVER_K,\n",
        "            \"lambda_mult\": Config.RETRIEVER_LAMBDA_MULT\n",
        "        }\n",
        "    )\n",
        "    print(\"âœ“ Retriever configured with MMR search\")\n",
        "    return retriever\n",
        "\n",
        "# Create vector store and retriever\n",
        "chroma_db = create_vector_store(pages)\n",
        "retriever = create_retriever(chroma_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Creating RAG Chain\n",
        "\n",
        "Build a Retrieval-Augmented Generation (RAG) chain to generate structured content from the PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_content_generation_chain(retriever, model):\n",
        "    \"\"\"\n",
        "    Create a RAG chain for generating structured bullet points.\n",
        "    \n",
        "    Args:\n",
        "        retriever: Document retriever\n",
        "        model: Language model to use\n",
        "        \n",
        "    Returns:\n",
        "        Configured chain\n",
        "    \"\"\"\n",
        "    template = \"\"\"\n",
        "    You are an expert at summarizing documents into clear, structured presentations.\n",
        "    \n",
        "    Given the context below, generate:\n",
        "    1. A clear, descriptive header\n",
        "    2. Exactly 10 numbered bullet points\n",
        "    3. Each bullet point should be 30-40 words\n",
        "    4. Focus on the most important information\n",
        "    \n",
        "    Format:\n",
        "    **Header: [Your Header Here]**\n",
        "    \n",
        "    1. **[Topic]**: [Description]\n",
        "    2. **[Topic]**: [Description]\n",
        "    ...\n",
        "    \n",
        "    Context: {context}\n",
        "    \n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    \n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    parser = StrOutputParser()\n",
        "    \n",
        "    chain = (\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | model\n",
        "        | parser\n",
        "    )\n",
        "    \n",
        "    return chain\n",
        "\n",
        "# Initialize model and chain\n",
        "chat_model = ChatOpenAI(model=Config.CHAT_MODEL)\n",
        "content_chain = create_content_generation_chain(retriever, chat_model)\n",
        "\n",
        "print(\"âœ“ RAG chain created successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate content\n",
        "query = \"What are the key points and implications of the briefing?\"\n",
        "print(f\"Query: {query}\\n\")\n",
        "\n",
        "response = content_chain.invoke(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generating PPTX Code\n",
        "\n",
        "Use an LLM to generate Python code for creating the PowerPoint presentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_code_generation_chain(presentation_title: str, output_filename: str):\n",
        "    \"\"\"\n",
        "    Create a chain for generating python-pptx code.\n",
        "    \n",
        "    Args:\n",
        "        presentation_title: Title for the presentation\n",
        "        output_filename: Name of the output PPTX file\n",
        "        \n",
        "    Returns:\n",
        "        Configured chain\n",
        "    \"\"\"\n",
        "    template = \"\"\"\n",
        "    You are an expert Python developer specializing in the python-pptx library.\n",
        "    \n",
        "    Task: Generate clean, executable Python code to create a PowerPoint presentation.\n",
        "    \n",
        "    Requirements:\n",
        "    1. Import required modules: `from pptx import Presentation` and `from pptx.util import Pt`\n",
        "    2. Create presentation with:\n",
        "       - Slide 1 (layout 0): Title: \"{title}\", Subtitle: \"Generated by AI\"\n",
        "       - Slide 2 (layout 1): Title: \"Key Insights (Part 1)\", Content: First 5 bullet points\n",
        "       - Slide 3 (layout 1): Title: \"Key Insights (Part 2)\", Content: Last 5 bullet points\n",
        "    3. Set body text font size to 18pt for readability\n",
        "    4. Save to: \"{output_path}\"\n",
        "    5. Output ONLY executable Python code, NO markdown formatting\n",
        "    6. Add error handling for file operations\n",
        "    \n",
        "    Content to include:\n",
        "    {context}\n",
        "    \n",
        "    Output format: Plain Python code only, no ```python``` markers.\n",
        "    \"\"\"\n",
        "    \n",
        "    output_path = Config.PPTX_DIR / output_filename\n",
        "    \n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    prompt = prompt.partial(\n",
        "        title=presentation_title,\n",
        "        output_path=str(output_path)\n",
        "    )\n",
        "    \n",
        "    model = OpenAI(\n",
        "        temperature=Config.CODE_GEN_TEMPERATURE,\n",
        "        max_tokens=Config.CODE_GEN_MAX_TOKENS\n",
        "    )\n",
        "    parser = StrOutputParser()\n",
        "    \n",
        "    chain = prompt | model | parser\n",
        "    return chain\n",
        "\n",
        "# Create code generation chain\n",
        "code_chain = create_code_generation_chain(\n",
        "    presentation_title=\"EPRS Briefing Analysis\",\n",
        "    output_filename=\"Red_Sea_Security_Threats.pptx\"\n",
        ")\n",
        "\n",
        "print(\"âœ“ Code generation chain created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Python code\n",
        "generated_code = code_chain.invoke({\"context\": response})\n",
        "print(\"Generated code preview:\")\n",
        "print(\"=\" * 80)\n",
        "print(generated_code[:500] + \"...\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Creating the Presentation\n",
        "\n",
        "Execute the generated code with proper validation and error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_python_code(code_str: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove markdown code block syntax from generated code.\n",
        "    \n",
        "    Args:\n",
        "        code_str: Raw code string from LLM\n",
        "        \n",
        "    Returns:\n",
        "        Cleaned Python code\n",
        "    \"\"\"\n",
        "    # Remove markdown code blocks\n",
        "    if '```python' in code_str:\n",
        "        code_str = code_str.split('```python')[1]\n",
        "    if '```' in code_str:\n",
        "        code_str = code_str.split('```')[0]\n",
        "    \n",
        "    return code_str.strip()\n",
        "\n",
        "def validate_code_safety(code: str) -> tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Perform basic safety checks on generated code.\n",
        "    \n",
        "    Args:\n",
        "        code: The code to validate\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (is_safe, message)\n",
        "    \"\"\"\n",
        "    dangerous_patterns = [\n",
        "        \"os.system\",\n",
        "        \"subprocess\",\n",
        "        \"eval(\",\n",
        "        \"__import__\",\n",
        "        \"open(\",  # Should only save PPTX files\n",
        "    ]\n",
        "    \n",
        "    # Check for dangerous patterns (excluding open() in context of pptx.save)\n",
        "    for pattern in dangerous_patterns:\n",
        "        if pattern == \"open(\" and \".save(\" in code:\n",
        "            continue  # Allow pptx save operations\n",
        "        if pattern in code:\n",
        "            return False, f\"Potentially unsafe code detected: {pattern}\"\n",
        "    \n",
        "    # Verify required imports are present\n",
        "    required_imports = [\"from pptx import Presentation\"]\n",
        "    for req in required_imports:\n",
        "        if req not in code:\n",
        "            return False, f\"Missing required import: {req}\"\n",
        "    \n",
        "    return True, \"Code validation passed\"\n",
        "\n",
        "def execute_generated_code(code: str, verbose: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Safely execute the generated Python code.\n",
        "    \n",
        "    Args:\n",
        "        code: The Python code to execute\n",
        "        verbose: Whether to print execution details\n",
        "        \n",
        "    Returns:\n",
        "        True if execution succeeded, False otherwise\n",
        "    \"\"\"\n",
        "    # Clean the code\n",
        "    cleaned_code = clean_python_code(code)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Cleaned code:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(cleaned_code)\n",
        "        print(\"=\" * 80)\n",
        "    \n",
        "    # Validate code safety\n",
        "    is_safe, message = validate_code_safety(cleaned_code)\n",
        "    if not is_safe:\n",
        "        print(f\"âœ— {message}\")\n",
        "        print(\"Code execution blocked for safety reasons.\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"âœ“ {message}\")\n",
        "    \n",
        "    # Execute the code\n",
        "    try:\n",
        "        exec(cleaned_code)\n",
        "        print(\"âœ“ Presentation created successfully!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error executing code: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Execute the generated code\n",
        "success = execute_generated_code(generated_code, verbose=False)\n",
        "\n",
        "if success:\n",
        "    output_file = Config.PPTX_DIR / \"Red_Sea_Security_Threats.pptx\"\n",
        "    print(f\"\\nðŸ“Š Presentation saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This improved notebook demonstrates:\n",
        "- âœ“ Configuration management with a Config class\n",
        "- âœ“ Proper error handling and validation\n",
        "- âœ“ Code organization with reusable functions\n",
        "- âœ“ Security checks for generated code\n",
        "- âœ“ Better documentation and type hints\n",
        "- âœ“ Modular design for easier testing and maintenance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
